{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e301b9a-21f0-4af3-8d9a-941c13dbf5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, ACCURACY: 0.3333333333333333, LOSS: 8.40744717002989\n",
      "EPOCH: 20, ACCURACY: 0.4, LOSS: 0.9215854842299206\n",
      "EPOCH: 40, ACCURACY: 0.43333333333333335, LOSS: 0.7536116948198671\n",
      "EPOCH: 60, ACCURACY: 0.42, LOSS: 0.6714779014672261\n",
      "EPOCH: 80, ACCURACY: 0.41333333333333333, LOSS: 0.6594143979448671\n",
      "EPOCH: 100, ACCURACY: 0.6666666666666666, LOSS: 0.5259943503852046\n",
      "EPOCH: 120, ACCURACY: 0.6666666666666666, LOSS: 0.4706373583820735\n",
      "EPOCH: 140, ACCURACY: 0.6666666666666666, LOSS: 0.5053203560733265\n",
      "EPOCH: 160, ACCURACY: 0.48, LOSS: 1.0150613941350848\n",
      "EPOCH: 180, ACCURACY: 0.8333333333333334, LOSS: 0.46065855297586067\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def relu(self, inputs):\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        exp_scores = np.exp(inputs)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def relu_derivative(self, dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    \n",
    "    def forward(self, inputs, weights, bias, activation):\n",
    "        Z_curr = np.dot(inputs, weights.T) + bias\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            A_curr = self.relu(inputs=Z_curr)\n",
    "        elif activation == 'softmax':\n",
    "            A_curr = self.softmax(inputs=Z_curr)\n",
    "            \n",
    "        return A_curr, Z_curr\n",
    "    \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
    "        if activation == 'softmax':\n",
    "            dW = np.dot(A_prev.T, dA_curr)\n",
    "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
    "            dA = np.dot(dA_curr, W_curr) \n",
    "        else:\n",
    "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
    "            dW = np.dot(A_prev.T, dZ)\n",
    "            db = np.sum(dZ, axis=0, keepdims=True)\n",
    "            dA = np.dot(dZ, W_curr)\n",
    "            \n",
    "        return dA, dW, db\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.network = [] ## layers\n",
    "        self.architecture = [] ## mapping input neurons --> output neurons\n",
    "        self.params = [] ## W, b\n",
    "        self.memory = [] ## Z, A\n",
    "        self.gradients = [] ## dW, db\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.network.append(layer)\n",
    "            \n",
    "    def _compile(self, data):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            if idx == 0:\n",
    "                self.architecture.append({'input_dim':data.shape[1], 'output_dim':self.network[idx].neurons,\n",
    "                                         'activation':'relu'})\n",
    "            elif idx > 0 and idx < len(self.network)-1:\n",
    "                self.architecture.append({'input_dim':self.network[idx-1].neurons, 'output_dim':self.network[idx].neurons,\n",
    "                                         'activation':'relu'})\n",
    "            else:\n",
    "                self.architecture.append({'input_dim':self.network[idx-1].neurons, 'output_dim':self.network[idx].neurons,\n",
    "                                         'activation':'softmax'})\n",
    "        return self\n",
    "    \n",
    "    def _init_weights(self, data):\n",
    "        self._compile(data)\n",
    "        \n",
    "        np.random.seed(99)\n",
    "        \n",
    "        for i in range(len(self.architecture)):\n",
    "            self.params.append({\n",
    "                'W':np.random.uniform(low=-1, high=1, \n",
    "                  size=(self.architecture[i]['output_dim'], \n",
    "                        self.architecture[i]['input_dim'])),\n",
    "                'b':np.zeros((1, self.architecture[i]['output_dim']))})\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _forwardprop(self, data):\n",
    "        A_curr = data\n",
    "        \n",
    "        for i in range(len(self.params)):\n",
    "            A_prev = A_curr\n",
    "            A_curr, Z_curr = self.network[i].forward(inputs=A_prev, weights=self.params[i]['W'], \n",
    "                                           bias=self.params[i]['b'], activation=self.architecture[i]['activation'])\n",
    "            \n",
    "            self.memory.append({'inputs':A_prev, 'Z':Z_curr})\n",
    "            \n",
    "        return A_curr\n",
    "    \n",
    "    def _backprop(self, predicted, actual):\n",
    "        num_samples = len(actual)\n",
    "        \n",
    "        ## compute the gradient on predictions\n",
    "        dscores = predicted\n",
    "        dscores[range(num_samples),actual] -= 1\n",
    "        dscores /= num_samples\n",
    "        \n",
    "        dA_prev = dscores\n",
    "        \n",
    "        for idx, layer in reversed(list(enumerate(self.network))):\n",
    "            dA_curr = dA_prev\n",
    "            \n",
    "            A_prev = self.memory[idx]['inputs']\n",
    "            Z_curr = self.memory[idx]['Z']\n",
    "            W_curr = self.params[idx]['W']\n",
    "            \n",
    "            activation = self.architecture[idx]['activation']\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr, Z_curr, A_prev, activation)\n",
    "\n",
    "            self.gradients.append({'dW':dW_curr, 'db':db_curr})\n",
    "            \n",
    "    def _update(self, lr=0.01):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            self.params[idx]['W'] -= lr * list(reversed(self.gradients))[idx]['dW'].T  \n",
    "            self.params[idx]['b'] -= lr * list(reversed(self.gradients))[idx]['db']\n",
    "    \n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
    "    \n",
    "    def _calculate_loss(self, predicted, actual):\n",
    "        samples = len(actual)\n",
    "        \n",
    "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
    "        data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs):\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        \n",
    "        self._init_weights(X_train)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            yhat = self._forwardprop(X_train)\n",
    "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
    "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
    "            \n",
    "            self._backprop(predicted=yhat, actual=y_train)\n",
    "            \n",
    "            self._update()\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
    "                print(s)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    def get_data(path):\n",
    "        data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "        cols = list(data.columns)\n",
    "        target = cols.pop()\n",
    "\n",
    "        X = data[cols].copy()\n",
    "        y = data[target].copy()\n",
    "\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X, y = get_data(r'iris.csv')\n",
    "\n",
    "    model = Network()\n",
    "    model.add(DenseLayer(6))\n",
    "    model.add(DenseLayer(8))\n",
    "    model.add(DenseLayer(10))\n",
    "    model.add(DenseLayer(3))\n",
    "\n",
    "    model.train(X_train=X, y_train=y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c1236d-2e33-42a7-b4b3-f6998d9a8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 8\n",
    "hidden_size2 = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)\n",
    "weights_input_hidden1 = np.random.randn(input_size, hidden_size1)\n",
    "bias_hidden1 = np.zeros((1, hidden_size1))\n",
    "weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "bias_hidden2 = np.zeros((1, hidden_size2))\n",
    "weights_hidden2_output = np.random.randn(hidden_size2, output_size)\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d28290c7-ef20-4742-9a47-ca41ded06e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8)\n",
      "(100, 8)\n",
      "(100, 1)\n",
      "0.7221645680206485\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,100) and (1,8) not aligned: 100 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     18\u001b[0m d_output \u001b[38;5;241m=\u001b[39m output_prob \u001b[38;5;241m-\u001b[39m y\n\u001b[1;32m---> 19\u001b[0m d_hidden2 \u001b[38;5;241m=\u001b[39m \u001b[43md_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_hidden2_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sigmoid_derivative(hidden2_output)\n\u001b[0;32m     20\u001b[0m d_hidden1 \u001b[38;5;241m=\u001b[39m d_hidden2\u001b[38;5;241m.\u001b[39mdot(weights_hidden1_hidden2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m sigmoid_derivative(hidden1_output)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,100) and (1,8) not aligned: 100 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "hidden1_input = np.dot(X, weights_input_hidden1) + bias_hidden1\n",
    "hidden1_output = sigmoid(hidden1_input)\n",
    "print(hidden1_output.shape)\n",
    "\n",
    "hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "hidden2_output = sigmoid(hidden2_input)\n",
    "print(hidden2_output.shape)\n",
    "\n",
    "output = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "output_prob = sigmoid(output)\n",
    "print(output_prob.shape)\n",
    "\n",
    "loss = -np.mean(y * np.log(output_prob) + (1 - y) * np.log(1 - output_prob)) # cross entropy loss\n",
    "losses.append(loss)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "d_output = output_prob - y\n",
    "d_hidden2 = d_output.dot(weights_hidden2_output.T) * sigmoid_derivative(hidden2_output)\n",
    "d_hidden1 = d_hidden2.dot(weights_hidden1_hidden2.T) * sigmoid_derivative(hidden1_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdc3a5-87c9-4e58-a46c-06ce1307948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(X, weights_input_hidden1) + bias_hidden1\n",
    "    hidden1_output = sigmoid(hidden1_input)\n",
    "    \n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "    hidden2_output = sigmoid(hidden2_input)\n",
    "    \n",
    "    output = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "    output_prob = sigmoid(output)\n",
    "\n",
    "    # Calculate the loss (binary cross-entropy)\n",
    "    loss = -np.mean(y * np.log(output_prob) + (1 - y) * np.log(1 - output_prob))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = output_prob - y\n",
    "    d_hidden2 = d_output.dot(weights_hidden2_output.T) * sigmoid_derivative(hidden2_output)\n",
    "    d_hidden1 = d_hidden2.dot(weights_hidden1_hidden2.T) * sigmoid_derivative(hidden1_output)\n",
    "\n",
    "    weights_hidden2_output -= hidden2_output.T.dot(d_output) * learning_rate\n",
    "    bias_output -= np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    weights_hidden1_hidden2 -= hidden1_output.T.dot(d_hidden2) * learning_rate\n",
    "    bias_hidden2 -= np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate\n",
    "    weights_input_hidden1 -= X.T.dot(d_hidden1) * learning_rate\n",
    "    bias_hidden1 -= np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Generate some test data for predictions\n",
    "X_test = np.array([[1.2, 1.3], [-1.2, -1.3]])\n",
    "\n",
    "# Forward propagation for test data\n",
    "hidden1_input_test = np.dot(X_test, weights_input_hidden1) + bias_hidden1\n",
    "hidden1_output_test = sigmoid(hidden1_input_test)\n",
    "\n",
    "hidden2_input_test = np.dot(hidden1_output_test, weights_hidden1_hidden2) + bias_hidden2\n",
    "hidden2_output_test = sigmoid(hidden2_input_test)\n",
    "\n",
    "output_test = np.dot(hidden2_output_test, weights_hidden2_output) + bias_output\n",
    "output_prob_test = sigmoid(output_test)\n",
    "\n",
    "print(\"Predictions for test data:\")\n",
    "print(output_prob_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed2c4081-d375-4aff-9bd9-1902354003d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,100) and (1,8) not aligned: 100 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     53\u001b[0m d_output \u001b[38;5;241m=\u001b[39m output_prob \u001b[38;5;241m-\u001b[39m y\n\u001b[1;32m---> 54\u001b[0m d_hidden2 \u001b[38;5;241m=\u001b[39m \u001b[43md_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_hidden2_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sigmoid_derivative(hidden2_output)\n\u001b[0;32m     55\u001b[0m d_hidden1 \u001b[38;5;241m=\u001b[39m d_hidden2\u001b[38;5;241m.\u001b[39mdot(weights_hidden1_hidden2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m sigmoid_derivative(hidden1_output)\n\u001b[0;32m     57\u001b[0m weights_hidden2_output \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m hidden2_output\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(d_output) \u001b[38;5;241m*\u001b[39m learning_rate\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,100) and (1,8) not aligned: 100 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some random data for training\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] * X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 8\n",
    "hidden_size2 = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)\n",
    "weights_input_hidden1 = np.random.randn(input_size, hidden_size1)\n",
    "bias_hidden1 = np.zeros((1, hidden_size1))\n",
    "weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "bias_hidden2 = np.zeros((1, hidden_size2))\n",
    "weights_hidden2_output = np.random.randn(hidden_size2, output_size)\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(X, weights_input_hidden1) + bias_hidden1\n",
    "    hidden1_output = sigmoid(hidden1_input)\n",
    "    \n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "    hidden2_output = sigmoid(hidden2_input)\n",
    "    \n",
    "    output = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "    output_prob = sigmoid(output)\n",
    "\n",
    "    # Calculate the loss (binary cross-entropy)\n",
    "    loss = -np.mean(y * np.log(output_prob) + (1 - y) * np.log(1 - output_prob))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = output_prob - y\n",
    "    d_hidden2 = d_output.dot(weights_hidden2_output.T) * sigmoid_derivative(hidden2_output)\n",
    "    d_hidden1 = d_hidden2.dot(weights_hidden1_hidden2.T) * sigmoid_derivative(hidden1_output)\n",
    "\n",
    "    weights_hidden2_output -= hidden2_output.T.dot(d_output) * learning_rate\n",
    "    bias_output -= np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    weights_hidden1_hidden2 -= hidden1_output.T.dot(d_hidden2) * learning_rate\n",
    "    bias_hidden2 -= np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate\n",
    "    weights_input_hidden1 -= X.T.dot(d_hidden1) * learning_rate\n",
    "    bias_hidden1 -= np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Generate some test data for predictions\n",
    "X_test = np.array([[1.2, 1.3], [-1.2, -1.3]])\n",
    "\n",
    "# Forward propagation for test data\n",
    "hidden1_input_test = np.dot(X_test, weights_input_hidden1) + bias_hidden1\n",
    "hidden1_output_test = sigmoid(hidden1_input_test)\n",
    "\n",
    "hidden2_input_test = np.dot(hidden1_output_test, weights_hidden1_hidden2) + bias_hidden2\n",
    "hidden2_output_test = sigmoid(hidden2_input_test)\n",
    "\n",
    "output_test = np.dot(hidden2_output_test, weights_hidden2_output) + bias_output\n",
    "output_prob_test = sigmoid(output_test)\n",
    "\n",
    "print(\"Predictions for test data:\")\n",
    "print(output_prob_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f98b3-8c05-4103-8e12-ebfd2d686cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
