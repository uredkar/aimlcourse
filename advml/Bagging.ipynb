{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990bc294-9a66-484d-8624-4c24a8df9d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a sample dataset (Iris dataset in this example)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a base classifier (Decision Tree in this example)\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Fit the BaggingClassifier on the training data\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a72166-5b73-41b2-ac2d-2db60e48df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a synthetic dataset (binary classification)\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # 100 samples with 2 features\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary target variable\n",
    "\n",
    "# Number of base classifiers (trees) in the ensemble\n",
    "num_classifiers = 5\n",
    "\n",
    "# Number of data points to use for each bootstrap sample\n",
    "bootstrap_size = len(X)\n",
    "\n",
    "# Create an ensemble of Decision Tree classifiers\n",
    "classifiers = []\n",
    "\n",
    "for _ in range(num_classifiers):\n",
    "    # Create a bootstrap sample by randomly selecting data points with replacement\n",
    "    bootstrap_indices = np.random.choice(range(bootstrap_size), size=bootstrap_size, replace=True)\n",
    "    X_bootstrap = X[bootstrap_indices]\n",
    "    y_bootstrap = y[bootstrap_indices]\n",
    "    \n",
    "    # Create and train a Decision Tree classifier on the bootstrap sample\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    classifier.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    # Append the trained classifier to the ensemble\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "# Make predictions using each classifier and aggregate the results by majority voting\n",
    "def ensemble_predict(classifiers, X):\n",
    "    predictions = np.array([classifier.predict(X) for classifier in classifiers])\n",
    "    # Use np.mean to obtain the majority vote (mode) of the predictions\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    return ensemble_predictions.round().astype(int)\n",
    "\n",
    "# Test the ensemble on a new dataset\n",
    "new_data_point = np.array([[0.6, 0.4]])  # A new data point to classify\n",
    "ensemble_prediction = ensemble_predict(classifiers, new_data_point)\n",
    "print(\"Ensemble Prediction:\", ensemble_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cab6fef-e1b1-426a-9878-705aa2242282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a synthetic dataset (binary classification)\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # 100 samples with 2 features\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary target variable\n",
    "\n",
    "# Number of base classifiers (trees) in the ensemble\n",
    "num_classifiers = 5\n",
    "\n",
    "# Create an ensemble of Random Forest classifiers\n",
    "classifiers = []\n",
    "\n",
    "for _ in range(num_classifiers):\n",
    "    # Create and train a Random Forest classifier\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(X, y)  # No need to create bootstrap samples\n",
    "    \n",
    "    # Append the trained classifier to the ensemble\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "# Make predictions using each classifier and aggregate the results by majority voting\n",
    "def ensemble_predict(classifiers, X):\n",
    "    predictions = np.array([classifier.predict(X) for classifier in classifiers])\n",
    "    # Use np.mean to obtain the majority vote (mode) of the predictions\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    return ensemble_predictions.round().astype(int)\n",
    "\n",
    "# Test the ensemble on a new dataset\n",
    "new_data_point = np.array([[0.6, 0.4]])  # A new data point to classify\n",
    "ensemble_prediction = ensemble_predict(classifiers, new_data_point)\n",
    "print(\"Ensemble Prediction:\", ensemble_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbc9f5f-7e54-4d63-b262-d39b4858f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class RandomForestClassifierCustom:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.ensemble = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Create a bootstrap sample\n",
    "            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_bootstrap = X[bootstrap_indices]\n",
    "            y_bootstrap = y[bootstrap_indices]\n",
    "\n",
    "            # Train a decision tree on the bootstrap sample\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "            )\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "            # Add the trained tree to the ensemble\n",
    "            self.ensemble.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using each tree in the ensemble\n",
    "        predictions = np.array([tree.predict(X) for tree in self.ensemble])\n",
    "\n",
    "        # Aggregate predictions by majority voting\n",
    "        ensemble_predictions = np.mean(predictions, axis=0)\n",
    "        return (ensemble_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Load a sample dataset (Iris dataset in this example)\n",
    "    data = load_iris()\n",
    "    X, y = data.data, (data.target == 2).astype(int)  # Binary classification\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create and train the custom Random Forest classifier\n",
    "    custom_rf = RandomForestClassifierCustom(n_estimators=100)\n",
    "    custom_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = custom_rf.predict(X_test)\n",
    "\n",
    "    # Calculate and print the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83659b4-f1e9-4be5-a86d-0315bfcd776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Bag Error: 0.35238095238095235\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class RandomForestClassifierCustom:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.ensemble = []\n",
    "        self.oob_predictions = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.oob_predictions = np.zeros((n_samples, self.n_estimators))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Create a bootstrap sample\n",
    "            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_bootstrap = X[bootstrap_indices]\n",
    "            y_bootstrap = y[bootstrap_indices]\n",
    "\n",
    "            # Train a decision tree on the bootstrap sample\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "            )\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "            # Add the trained tree to the ensemble\n",
    "            self.ensemble.append(tree)\n",
    "\n",
    "            # Record OOB predictions for this tree\n",
    "            oob_indices = np.setdiff1d(np.arange(n_samples), bootstrap_indices)\n",
    "            oob_predictions_i = tree.predict(X[oob_indices])\n",
    "            self.oob_predictions[oob_indices, i] = oob_predictions_i\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using each tree in the ensemble\n",
    "        predictions = np.array([tree.predict(X) for tree in self.ensemble])\n",
    "\n",
    "        # Aggregate predictions by majority voting\n",
    "        ensemble_predictions = np.mean(predictions, axis=0)\n",
    "        return (ensemble_predictions >= 0.5).astype(int)\n",
    "\n",
    "    def oob_error(self, X, y):\n",
    "        # Calculate the OOB error\n",
    "        predicted_classes = np.round(self.oob_predictions.mean(axis=1)).astype(int)\n",
    "        return 1.0 - np.mean(predicted_classes == y)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load a sample dataset (Iris dataset in this example)\n",
    "    data = load_iris()\n",
    "    X, y = data.data, (data.target == 2).astype(int)  # Binary classification\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create and train the custom Random Forest classifier\n",
    "    custom_rf = RandomForestClassifierCustom(n_estimators=100)\n",
    "    custom_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate and print the out-of-bag error\n",
    "    oob_error = custom_rf.oob_error(X_train, y_train)\n",
    "    print(\"Out-of-Bag Error:\", oob_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f43363a-f46c-4195-a330-ea0be7070a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       270\n",
      "           1       0.78      0.47      0.58        30\n",
      "\n",
      "    accuracy                           0.93       300\n",
      "   macro avg       0.86      0.73      0.77       300\n",
      "weighted avg       0.93      0.93      0.93       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calculate class weights (inversely proportional to class frequencies)\n",
    "class_weights = {0: 1.0, 1: 10.0}  # Adjust the weights as needed based on the class imbalance\n",
    "\n",
    "# Create a Random Forest classifier with class weights\n",
    "rf_classifier = RandomForestClassifier(class_weight=class_weights, random_state=42)\n",
    "\n",
    "# Train the model using oversampling with class weights\n",
    "# This will create synthetic samples for the minority class\n",
    "X_train_oversampled, y_train_oversampled = X_train, y_train  # Initialize with the original training data\n",
    "\n",
    "# Find the minority class samples\n",
    "minority_class_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# Oversample the minority class to match the majority class size\n",
    "oversampling_factor = int(class_weights[0] / class_weights[1])\n",
    "for _ in range(oversampling_factor - 1):\n",
    "    X_train_oversampled = np.vstack((X_train_oversampled, X_train[minority_class_indices]))\n",
    "    y_train_oversampled = np.hstack((y_train_oversampled, y_train[minority_class_indices]))\n",
    "\n",
    "# Train the model on the oversampled training data\n",
    "rf_classifier.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e214534-6dec-450e-bb0c-b9f6d0e78ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def process_classweights(minority_class, data, balance_percentage):\n",
    "    \"\"\"\n",
    "    Balances a dataset by oversampling the minority class to a specified percentage.\n",
    "\n",
    "    Parameters:\n",
    "    - minority_class: int\n",
    "        The class label of the minority class.\n",
    "    - data: numpy array or pandas DataFrame\n",
    "        The input dataset with features and labels.\n",
    "    - balance_percentage: float (0 to 1)\n",
    "        The desired percentage of the minority class samples in the balanced dataset.\n",
    "\n",
    "    Returns:\n",
    "    - balanced_data: numpy array or pandas DataFrame\n",
    "        The balanced dataset with the specified percentage of the minority class.\n",
    "    \"\"\"\n",
    "    # Separate the dataset into features (X) and labels (y)\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        X = data.drop(columns='label').values\n",
    "        y = data['label'].values\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        X = data[:, :-1]\n",
    "        y = data[:, -1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type. Use a numpy array or pandas DataFrame.\")\n",
    "\n",
    "    # Find indices of the minority class samples\n",
    "    minority_indices = np.where(y == minority_class)[0]\n",
    "\n",
    "    # Calculate the number of minority class samples to achieve the desired balance percentage\n",
    "    num_minority_samples_needed = int(balance_percentage * len(minority_indices) / (1 - balance_percentage))\n",
    "\n",
    "    # Oversample the minority class to meet the desired balance percentage\n",
    "    oversampled_indices = np.random.choice(minority_indices, size=num_minority_samples_needed, replace=True)\n",
    "    \n",
    "    # Combine the oversampled minority class samples with the majority class samples\n",
    "    balanced_indices = np.concatenate((oversampled_indices, np.where(y != minority_class)[0]))\n",
    "\n",
    "    # Create the balanced dataset\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        balanced_data = data.iloc[balanced_indices]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        balanced_data = np.hstack((X[balanced_indices], y[balanced_indices].reshape(-1, 1)))\n",
    "    else:\n",
    "        balanced_data = None\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "# Example usage:\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "balanced_data = process_classweights(minority_class=1, data=X, balance_percentage=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3c6141c-9f64-4271-bd4c-fd51f1647f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6693561 , -0.19806908, -0.87076638, ..., -1.26733697,\n",
       "         0.26173564,  1.01664321],\n",
       "       [ 0.09337237,  0.78584826,  0.10575379, ..., -0.12270893,\n",
       "         0.6934308 ,  0.91136272],\n",
       "       [-0.90579721,  1.03575674,  0.29514098, ...,  0.83049813,\n",
       "         0.95404926, -0.5782121 ],\n",
       "       ...,\n",
       "       [-0.20013455, -1.46108168,  1.79701652, ..., -1.50280171,\n",
       "        -1.27473745,  1.60111869],\n",
       "       [ 0.03935575,  0.24868361, -0.47532342, ...,  0.09912579,\n",
       "         0.54269228,  1.20827474],\n",
       "       [ 0.76921528,  0.47076539,  0.16994471, ...,  0.6561162 ,\n",
       "         0.64333186, -2.02100232]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7d94bd-4047-4453-80f9-6110ffa3ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Bag Error: 4.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Access the OOB error score\n",
    "oob_error = 1 - rf_classifier.oob_score_\n",
    "print(f\"Out-of-Bag Error: {oob_error:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66995f0e-e31a-49c4-aef4-f54437ef1718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
