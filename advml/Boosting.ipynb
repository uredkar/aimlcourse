{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c68fed-5e1b-4bd2-a342-9ab09a38bdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple decision tree class\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(set(y)) == 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        # Recursively build left and right subtrees\n",
    "        left_indices = best_split['left_indices']\n",
    "        right_indices = best_split['right_indices']\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature_idx': best_split['feature_idx'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree,\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_impurity = float('inf')\n",
    "\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature_idx] < threshold\n",
    "                right_indices = ~left_indices\n",
    "\n",
    "                if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                left_y = y[left_indices]\n",
    "                right_y = y[right_indices]\n",
    "\n",
    "                impurity = self._calculate_impurity(left_y, right_y)\n",
    "                if impurity < best_impurity:\n",
    "                    best_impurity = impurity\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': threshold,\n",
    "                        'left_indices': left_indices,\n",
    "                        'right_indices': right_indices,\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_impurity(self, left_y, right_y):\n",
    "        # Calculate impurity (e.g., mean squared error for regression)\n",
    "        return np.mean((left_y - np.mean(left_y)) ** 2) + np.mean((right_y - np.mean(right_y)) ** 2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            if x[tree['feature_idx']] < tree['threshold']:\n",
    "                return self._predict_single(x, tree['left'])\n",
    "            else:\n",
    "                return self._predict_single(x, tree['right'])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "# Implement Gradient Boosting\n",
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.mean(y)  # Initialize predictions with the mean of y\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the gradient (negative gradient for MSE)\n",
    "            gradient = -(y - y_pred)\n",
    "            # Fit a decision tree to the negative gradient\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, gradient)\n",
    "            # Update predictions using the new tree\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 1)\n",
    "    y = 2 * X[:, 0] + np.random.randn(100)\n",
    "\n",
    "    # Create and fit the Gradient Boosting model\n",
    "    gb = GradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    gb.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    y_pred = gb.predict(X_test)\n",
    "\n",
    "    # Plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.scatter(X, y, label=\"Actual\")\n",
    "    plt.plot(X_test, y_pred, label=\"Predicted\", color='red')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf556de-6292-4353-b696-accfe8e56339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple decision stump as the base learner\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.0\n",
    "        self.feature_idx = 0\n",
    "        self.alpha = 0.0\n",
    "\n",
    "    def fit(self, X, y, weights):\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float('inf')\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                predictions = np.ones(n_samples)\n",
    "                predictions[X[:, feature_idx] < threshold] = -1\n",
    "\n",
    "                error = np.sum(weights[y != predictions])\n",
    "\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    self.threshold = threshold\n",
    "                    self.feature_idx = feature_idx\n",
    "\n",
    "        self.alpha = 0.5 * np.log((1.0 - min_error) / (min_error + 1e-10))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.ones(n_samples)\n",
    "        predictions[X[:, self.feature_idx] < self.threshold] = -1\n",
    "        return predictions\n",
    "\n",
    "# Define the AdaBoost classifier\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = DecisionStump()\n",
    "            model.fit(X, y, weights)\n",
    "            predictions = model.predict(X)\n",
    "\n",
    "            weighted_error = np.sum(weights[y != predictions])\n",
    "\n",
    "            alpha = 0.5 * np.log((1.0 - weighted_error) / (weighted_error + 1e-10))\n",
    "            weights *= np.exp(-alpha * y * predictions)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        final_predictions = np.zeros(n_samples)\n",
    "\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            final_predictions += alpha * model.predict(X)\n",
    "\n",
    "        return np.sign(final_predictions)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 2) * 2 - 1\n",
    "    y = np.sign(X[:, 0] * X[:, 1])\n",
    "\n",
    "    # Create and train AdaBoost classifier\n",
    "    ada_boost = AdaBoost(n_estimators=50)\n",
    "    ada_boost.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = ada_boost.predict(X)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y == y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23e3960-fd81-4a7d-9fb5-cd7c024d0008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Accuracy: 0.90\n",
      "Decision Tree - Confusion Matrix:\n",
      "[[257  13]\n",
      " [ 18  12]]\n",
      "Random Forest - Accuracy: 0.93\n",
      "Random Forest - Confusion Matrix:\n",
      "[[266   4]\n",
      " [ 16  14]]\n",
      "Gradient Boosting - Accuracy: 0.95\n",
      "Gradient Boosting - Confusion Matrix:\n",
      "[[268   2]\n",
      " [ 12  18]]\n",
      "XGBoost - Accuracy: 0.95\n",
      "XGBoost - Confusion Matrix:\n",
      "[[267   3]\n",
      " [ 12  18]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Generate synthetic imbalanced data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define class weights\n",
    "class_weights = {0: 1.0, 1: 10.0}  # Adjust the weights as needed based on the class imbalance\n",
    "\n",
    "# Decision Tree with Class Weights\n",
    "tree_classifier = DecisionTreeClassifier(class_weight=class_weights, random_state=42)\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "tree_predictions = tree_classifier.predict(X_test)\n",
    "\n",
    "# Random Forest with Class Weights\n",
    "rf_classifier = RandomForestClassifier(class_weight=class_weights, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Gradient Boosting with Class Weights\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_predictions = gb_classifier.predict(X_test)\n",
    "\n",
    "# XGBoost with Class Weights\n",
    "xgb_classifier = xgb.XGBClassifier(scale_pos_weight=10.0, random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(predictions, model_name):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    confusion = confusion_matrix(y_test, predictions)\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"{model_name} - Confusion Matrix:\\n{confusion}\")\n",
    "\n",
    "evaluate_model(tree_predictions, \"Decision Tree\")\n",
    "evaluate_model(rf_predictions, \"Random Forest\")\n",
    "evaluate_model(gb_predictions, \"Gradient Boosting\")\n",
    "evaluate_model(xgb_predictions, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6db1e3-9dc6-42b3-9b77-fe5d19a89dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Model Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Base models\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "# Add more base models as needed\n",
    "\n",
    "# Meta-model (Logistic Regression in this example)\n",
    "meta_classifier = LogisticRegression()\n",
    "\n",
    "# Train the base models on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "# Train additional base models as needed\n",
    "\n",
    "# Generate predictions from the base models for the validation set\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "gb_predictions = gb_classifier.predict(X_test)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "# Generate predictions from additional base models as needed\n",
    "\n",
    "# Create a new feature matrix for the meta-model using base model predictions\n",
    "stacked_predictions = np.column_stack((rf_predictions, gb_predictions, knn_predictions))\n",
    "# Stack predictions from additional base models as needed\n",
    "\n",
    "# Train the meta-model on the stacked predictions\n",
    "meta_classifier.fit(stacked_predictions, y_test)\n",
    "\n",
    "# Now, make predictions using the base models and the meta-model for stacking\n",
    "rf_predictions_test = rf_classifier.predict(X_test)\n",
    "gb_predictions_test = gb_classifier.predict(X_test)\n",
    "knn_predictions_test = knn_classifier.predict(X_test)\n",
    "# Generate predictions from additional base models as needed\n",
    "\n",
    "stacked_test_predictions = np.column_stack((rf_predictions_test, gb_predictions_test, knn_predictions_test))\n",
    "# Stack predictions from additional base models as needed\n",
    "\n",
    "meta_predictions = meta_classifier.predict(stacked_test_predictions)\n",
    "\n",
    "# Evaluate the meta-model's performance\n",
    "meta_accuracy = accuracy_score(y_test, meta_predictions)\n",
    "print(f\"Meta-Model Accuracy: {meta_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4b9a33-b8d6-427e-b0a8-1adc3e4dcef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Iteration 2:\n",
      "Iteration 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]])\n",
    "y = np.array([-1, -1, 1, 1, 1])  # Labels (binary classification)\n",
    "\n",
    "# Initialize sample weights (uniformly distributed)\n",
    "sample_weights = np.ones(len(X)) / len(X)\n",
    "\n",
    "# Create an AdaBoostClassifier with DecisionTreeClassifier as the base estimator\n",
    "n_estimators = 3  # Number of weak classifiers (models)\n",
    "ada_boost_model = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=n_estimators\n",
    ")\n",
    "\n",
    "epsilon = 1e-10  # Small epsilon to avoid division by zero\n",
    "\n",
    "# Sequentially fit the AdaBoost model and show the weights after each iteration\n",
    "for i in range(n_estimators):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    \n",
    "    # Fit the AdaBoost model with the current sample weights\n",
    "    ada_boost_model.fit(X, y, sample_weight=sample_weights)\n",
    "    \n",
    "    # Calculate error rate for the current model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3bec93-8cbc-43de-b778-c96c3f85774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Model 1 Weight: 11.5129\n",
      "Sample Weights: [0.199998 0.199998 0.199998 0.199998 0.199998]\n",
      "Increased Weights for Misclassified Points: []\n",
      "\n",
      "Iteration 2:\n",
      "Model 2 Weight: 11.5129\n",
      "Sample Weights: [0.199998 0.199998 0.199998 0.199998 0.199998]\n",
      "Increased Weights for Misclassified Points: []\n",
      "\n",
      "Iteration 3:\n",
      "Model 3 Weight: 11.5129\n",
      "Sample Weights: [0.199998 0.199998 0.199998 0.199998 0.199998]\n",
      "Increased Weights for Misclassified Points: []\n",
      "\n",
      "Final Predictions: [-1 -1  1  1  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]])\n",
    "y = np.array([-1, -1, 1, 1, 1])  # Labels (binary classification)\n",
    "\n",
    "# Initialize sample weights (uniformly distributed)\n",
    "sample_weights = np.ones(len(X)) / len(X)\n",
    "\n",
    "# Create an AdaBoostClassifier with DecisionTreeClassifier as the base estimator\n",
    "n_estimators = 3  # Number of weak classifiers (models)\n",
    "ada_boost_model = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=n_estimators\n",
    ")\n",
    "\n",
    "epsilon = 1e-10  # Small epsilon to avoid division by zero\n",
    "\n",
    "# Sequentially fit the AdaBoost model and show the weights after each iteration\n",
    "for i in range(n_estimators):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    \n",
    "    # Fit the AdaBoost model with the current sample weights\n",
    "    ada_boost_model.fit(X, y, sample_weight=sample_weights)\n",
    "    \n",
    "    # Calculate error rate for the current model\n",
    "    y_pred = ada_boost_model.predict(X)\n",
    "    misclassified = (y_pred != y)\n",
    "    error_rate = np.sum(sample_weights * misclassified) / (np.sum(sample_weights) + epsilon)\n",
    "    \n",
    "    # Calculate the model weight (alpha)\n",
    "    alpha = 0.5 * np.log((1 - error_rate) / (error_rate + epsilon))\n",
    "    \n",
    "    # Update sample weights and print the indices of increased weights\n",
    "    for j in range(len(sample_weights)):\n",
    "        if misclassified[j]:\n",
    "            sample_weights[j] *= np.exp(alpha + epsilon)  # Increase weight for misclassified points\n",
    "        else:\n",
    "            sample_weights[j] *= np.exp(-alpha - epsilon)  # Decrease weight for correctly classified points\n",
    "    sample_weights /= (np.sum(sample_weights) + epsilon)  # Normalize\n",
    "    \n",
    "    print(f\"Model {i+1} Weight: {alpha:.4f}\")\n",
    "    print(f\"Sample Weights: {sample_weights}\")\n",
    "    print(f\"Increased Weights for Misclassified Points: {np.where(misclassified)[0]}\\n\")\n",
    "\n",
    "# Final AdaBoost model\n",
    "final_predictions = ada_boost_model.predict(X)\n",
    "print(\"Final Predictions:\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84345add-6a99-4ea4-b5a2-07bde96fbd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Model 1 Weight: 11.5129\n",
      "Sample Weights: [0.199998 0.199998 0.199998 0.199998 0.199998]\n",
      "Increased Weights for Misclassified Points: []\n",
      "\n",
      "Iteration 2:\n",
      "Model 2 Weight: 11.5129\n",
      "Sample Weights: [0.199998 0.199998 0.199998 0.199998 0.199998]\n",
      "Increased Weights for Misclassified Points: []\n",
      "\n",
      "Iteration 3:\n",
      "Model 3 Weight: 11.5129\n",
      "Sample Weights: [0.199998 0.199998 0.199998 0.199998 0.199998]\n",
      "Increased Weights for Misclassified Points: []\n",
      "\n",
      "Final Predictions (Weighted Voting): [-1. -1.  1.  1.  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]])\n",
    "y = np.array([-1, -1, 1, 1, 1])  # Labels (binary classification)\n",
    "\n",
    "# Initialize sample weights (uniformly distributed)\n",
    "sample_weights = np.ones(len(X)) / len(X)\n",
    "\n",
    "# Create an AdaBoostClassifier with DecisionTreeClassifier as the base estimator\n",
    "n_estimators = 3  # Number of weak classifiers (models)\n",
    "ada_boost_model = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=n_estimators\n",
    ")\n",
    "\n",
    "epsilon = 1e-10  # Small epsilon to avoid division by zero\n",
    "\n",
    "# Initialize an array to store predictions of each model in each iteration\n",
    "model_predictions = []\n",
    "\n",
    "# Sequentially fit the AdaBoost model and show the weights after each iteration\n",
    "for i in range(n_estimators):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    \n",
    "    # Fit the AdaBoost model with the current sample weights\n",
    "    ada_boost_model.fit(X, y, sample_weight=sample_weights)\n",
    "    \n",
    "    # Store the predictions of the current model\n",
    "    model_predictions.append(ada_boost_model.predict(X))\n",
    "    \n",
    "    # Calculate error rate for the current model\n",
    "    y_pred = ada_boost_model.predict(X)\n",
    "    misclassified = (y_pred != y)\n",
    "    error_rate = np.sum(sample_weights * misclassified) / (np.sum(sample_weights) + epsilon)\n",
    "    \n",
    "    # Calculate the model weight (alpha)\n",
    "    alpha = 0.5 * np.log((1 - error_rate) / (error_rate + epsilon))\n",
    "    \n",
    "    # Update sample weights and print the indices of increased weights\n",
    "    for j in range(len(sample_weights)):\n",
    "        if misclassified[j]:\n",
    "            sample_weights[j] *= np.exp(alpha + epsilon)  # Increase weight for misclassified points\n",
    "        else:\n",
    "            sample_weights[j] *= np.exp(-alpha - epsilon)  # Decrease weight for correctly classified points\n",
    "    sample_weights /= (np.sum(sample_weights) + epsilon)  # Normalize\n",
    "    \n",
    "    print(f\"Model {i+1} Weight: {alpha:.4f}\")\n",
    "    print(f\"Sample Weights: {sample_weights}\")\n",
    "    print(f\"Increased Weights for Misclassified Points: {np.where(misclassified)[0]}\\n\")\n",
    "\n",
    "# Compute the final prediction based on weighted voting\n",
    "final_predictions = np.zeros_like(y, dtype=float)\n",
    "for i, predictions in enumerate(model_predictions):\n",
    "    alpha = 0.5 * np.log((1 - error_rate) / (error_rate + epsilon))\n",
    "    final_predictions += alpha * predictions\n",
    "\n",
    "final_predictions = np.sign(final_predictions)  # Convert to binary predictions (-1 or 1)\n",
    "\n",
    "print(\"Final Predictions (Weighted Voting):\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2f5b6c-ffa4-4434-8f43-198bae83b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Residuals: [ 0.5  0.5 -0.5 -0.5 -0.5]\n",
      "Weak Learner Predictions: [ 0.5  0.5 -0.5 -0.5 -0.5]\n",
      "Updated Ensemble Predictions: [ 0.05  0.05 -0.05 -0.05 -0.05]\n",
      "\n",
      "Iteration 2:\n",
      "Residuals: [ 0.4875026  0.4875026 -0.4875026 -0.4875026 -0.4875026]\n",
      "Weak Learner Predictions: [ 0.4875026  0.4875026 -0.4875026 -0.4875026 -0.4875026]\n",
      "Updated Ensemble Predictions: [ 0.09875026  0.09875026 -0.09875026 -0.09875026 -0.09875026]\n",
      "\n",
      "Iteration 3:\n",
      "Residuals: [ 0.47533248  0.47533248 -0.47533248 -0.47533248 -0.47533248]\n",
      "Weak Learner Predictions: [ 0.47533248  0.47533248 -0.47533248 -0.47533248 -0.47533248]\n",
      "Updated Ensemble Predictions: [ 0.14628351  0.14628351 -0.14628351 -0.14628351 -0.14628351]\n",
      "\n",
      "Final Ensemble Predictions (Probabilities): [0.5365058 0.5365058 0.4634942 0.4634942 0.4634942]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]])\n",
    "y = np.array([1, 1, 0, 0, 0])  # Binary labels (0 or 1)\n",
    "\n",
    "# Initialize ensemble predictions\n",
    "ensemble_predictions = np.zeros(len(X))\n",
    "\n",
    "# Create a GradientBoostingClassifier\n",
    "n_estimators = 3  # Number of boosting stages (models)\n",
    "learning_rate = 0.1  # Learning rate\n",
    "gradient_boosting_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Fit the GradientBoostingClassifier and calculate residuals\n",
    "for i in range(n_estimators):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    \n",
    "    # Calculate residuals (negative gradient)\n",
    "    residuals = y - 1 / (1 + np.exp(-ensemble_predictions))\n",
    "    \n",
    "    # Fit a decision tree regressor to the residuals\n",
    "    weak_learner = DecisionTreeRegressor(max_depth=1)\n",
    "    weak_learner.fit(X, residuals)\n",
    "    \n",
    "    # Predictions of the weak learner\n",
    "    weak_predictions = weak_learner.predict(X)\n",
    "    \n",
    "    # Update the ensemble predictions with weighted weak predictions\n",
    "    ensemble_predictions += learning_rate * weak_predictions\n",
    "    \n",
    "    print(f\"Residuals: {residuals}\")\n",
    "    print(f\"Weak Learner Predictions: {weak_predictions}\")\n",
    "    print(f\"Updated Ensemble Predictions: {ensemble_predictions}\\n\")\n",
    "\n",
    "# Final Ensemble Predictions\n",
    "final_predictions = 1 / (1 + np.exp(-ensemble_predictions))  # Convert to probabilities\n",
    "print(\"Final Ensemble Predictions (Probabilities):\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7f9cd5-e91b-473c-8e24-6ac8b8c57f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2383: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "C:\\Users\\unmes\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000000\n",
      "         Iterations 32\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m logit_model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mLogit(y, X)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Perform the logistic regression\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m logit_result \u001b[38;5;241m=\u001b[39m \u001b[43mlogit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Print the summary of the logistic regression model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(logit_result\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2599\u001b[0m, in \u001b[0;36mLogit.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m   2596\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(DiscreteModel\u001b[38;5;241m.\u001b[39mfit\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m, maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m,\n\u001b[0;32m   2598\u001b[0m         full_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2599\u001b[0m     bnryfit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(start_params\u001b[38;5;241m=\u001b[39mstart_params,\n\u001b[0;32m   2600\u001b[0m                           method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   2601\u001b[0m                           maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m   2602\u001b[0m                           full_output\u001b[38;5;241m=\u001b[39mfull_output,\n\u001b[0;32m   2603\u001b[0m                           disp\u001b[38;5;241m=\u001b[39mdisp,\n\u001b[0;32m   2604\u001b[0m                           callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m   2605\u001b[0m                           \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2607\u001b[0m     discretefit \u001b[38;5;241m=\u001b[39m LogitResults(\u001b[38;5;28mself\u001b[39m, bnryfit)\n\u001b[0;32m   2608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:243\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(start_params\u001b[38;5;241m=\u001b[39mstart_params,\n\u001b[0;32m    244\u001b[0m                      method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    245\u001b[0m                      maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m    246\u001b[0m                      full_output\u001b[38;5;241m=\u001b[39mfull_output,\n\u001b[0;32m    247\u001b[0m                      disp\u001b[38;5;241m=\u001b[39mdisp,\n\u001b[0;32m    248\u001b[0m                      callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    249\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mlefit\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:582\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m     Hinv \u001b[38;5;241m=\u001b[39m cov_params_func(\u001b[38;5;28mself\u001b[39m, xopt, retvals)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m full_output:\n\u001b[1;32m--> 582\u001b[0m     Hinv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mretvals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHessian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_hessian:\n\u001b[0;32m    584\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhessian(xopt)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:538\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    536\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    537\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 538\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Generate some synthetic data for binary classification\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Create a binary target variable\n",
    "\n",
    "# Add a constant term (intercept) to the features\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "logit_model = sm.Logit(y, X)\n",
    "\n",
    "# Perform the logistic regression\n",
    "logit_result = logit_model.fit()\n",
    "\n",
    "# Print the summary of the logistic regression model\n",
    "print(logit_result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469282c5-5f5b-4c25-8451-961558518ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
