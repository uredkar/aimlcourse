{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRX9f5_6HUNP"
   },
   "source": [
    "# Introduction to Neural Networks: Bank Churn prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjPy2L-tqoCo"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ulaip3ctHj"
   },
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcLJz1kvqoq0"
   },
   "source": [
    "Businesses like banks which provide service have to worry about problem of 'Customer Churn' i.e. customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer's decision in this regard. Management can concentrate efforts on improvement of service, keeping in mind these priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfRtSZuqcvNP"
   },
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtljX1H-qqoD"
   },
   "source": [
    "You as a Data scientist with the  bank need to  build a neural network based classifier that can determine whether a customer will leave the bank  or not in the next 6 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VS6p3QCczwW"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99T0qx8mq2DK"
   },
   "source": [
    "* CustomerId: Unique ID which is assigned to each customer\n",
    "\n",
    "* Surname: Last name of the customer\n",
    " \n",
    "* CreditScore: It defines the credit history of the customer.\n",
    "  \n",
    "* Geography: A customer’s location \n",
    "   \n",
    "* Gender: It defines the Gender of the customer\n",
    "   \n",
    "* Age: Age of the customer \n",
    "    \n",
    "* Tenure: Number of years for which the customer has been with the bank\n",
    "\n",
    "* NumOfProducts: refers to the number of products that a customer has purchased through the bank.\n",
    "\n",
    "* Balance: Account balance\n",
    "\n",
    "* HasCrCard: It is a categorical variable which decides whether the customer has credit card or not.\n",
    "\n",
    "* EstimatedSalary: Estimated salary \n",
    "\n",
    "* isActiveMember: Is is a categorical variable which decides whether the customer is active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions etc )\n",
    "\n",
    "* Exited : whether or not the customer left the bank within six month. It can take two values \n",
    "** 0=No ( Customer did not leave the bank )\n",
    "** 1=Yes ( Customer left the bank )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjqhXJsbdCc5"
   },
   "source": [
    "## **Please read the instructions carefully before starting the project.** \n",
    "\n",
    "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned. \n",
    "\n",
    "* Blanks '_______' are provided in the notebook that need to be filled with an appropriate code to get the correct result\n",
    "\n",
    "* With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space\n",
    "\n",
    "* Identify the task to be performed correctly and only then proceed to write the required code\n",
    "\n",
    "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\"\n",
    "\n",
    "* Running incomplete code may throw an error\n",
    "\n",
    "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors\n",
    "\n",
    "* Add the results/observations derived from the analysis in the presentation and submit the same in .pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cYmf-q8c726"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IfeZclzIHUNs"
   },
   "outputs": [],
   "source": [
    "# Libraries to help with reading and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Library to encode the variables\n",
    "from sklearn import preprocessing\n",
    "# To plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# library to import to standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#To import different metrics \n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import backend\n",
    "# Library to avoid the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# importing different functions to build models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "# importing GridSearch CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# importing roc_curve to plot\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "# importing SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# importing metrics\n",
    "from sklearn import metrics\n",
    "import random\n",
    "#Importing classback API\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZJ5YA3KeePX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7ubXtC8HUOA"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1QJLp3P3HUOC"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbank_1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Complete the code to read the dataset \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv(\"bank_1.csv\")  # Complete the code to read the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwaZDbsYf0-N"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yBI-Ck4f_Yn"
   },
   "source": [
    "The initial steps to get an overview of any dataset is to: \n",
    "- Observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
    "- Get information about the number of rows and columns in the dataset\n",
    "- Find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
    "- Check the statistical summary of the dataset to get an overview of the numerical columns of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TZ5GstYgG1I"
   },
   "source": [
    "### View the first and last 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCfASHJ8HUOS"
   },
   "outputs": [],
   "source": [
    "ds.____() ## Complete the code to display the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGgv1iQKrbKX"
   },
   "outputs": [],
   "source": [
    "ds.____() ## Complete the code to display the last 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-Dl42UCgLdV"
   },
   "source": [
    "### Understand the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsyeCM0XgOJT"
   },
   "outputs": [],
   "source": [
    "ds.'_______' ## Complete the code to get the shape of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP6rrI-kgaxW"
   },
   "source": [
    "### Check the data types of the columns for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEdvRgtKgc-M"
   },
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGhe9qVhgoY1"
   },
   "source": [
    "### Checking the Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR9XpHgzgrEt"
   },
   "outputs": [],
   "source": [
    "ds.'_______' ## Complete the code to print the statistical summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TTT9Escruwg"
   },
   "source": [
    "### Checking for unique values for each of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SPdThJArvcG"
   },
   "outputs": [],
   "source": [
    "ds.'________________________' ## Complete the code to get the unique values in each of the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M_y7Y33g2nP"
   },
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwwwQTOAgyOo"
   },
   "outputs": [],
   "source": [
    "ds.'_______' ## Complete the code to check duplicate entries in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsBwLHcmHUOg"
   },
   "source": [
    "### Drop the columns which are unique for all users like IDs, names, and rownumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivF2RMo6HUOr"
   },
   "outputs": [],
   "source": [
    "# RowNumber , CustomerId and Surname are unique hence drop them\n",
    "ds = ds.drop(['_____', '_____', '_____'], axis=1)  # complete the code to drop the columns with most unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W036jsgwRdVN"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQlhG1x0iREg"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TCm_e-TiTra"
   },
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_dQi6vriVs2"
   },
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1WgW83rjza0"
   },
   "source": [
    "#### Observations on CreditScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOEYFUenjyNu"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot(ds,'CreditScore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUk3Aj_Bj3cl"
   },
   "source": [
    "#### Observations on Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5uykb4ZjyLI"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('__________________')          ## Complete the code to create histogram_boxplot for CreditScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ceasiVRkcjS"
   },
   "source": [
    "#### Observations on Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19oK6z4kjyGd"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('__________________')          ## Complete the code to create histogram_boxplot for Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rZ-1bs_ksSe"
   },
   "source": [
    "#### Observations on Estimated Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgsC0cimjyDk"
   },
   "outputs": [],
   "source": [
    "histogram_boxplot('__________________')          ## Complete the code to create histogram_boxplot for Estimated Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWM5q0wQkvws"
   },
   "source": [
    "#### Observations on Exited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YD9p3sPvjyA-"
   },
   "outputs": [],
   "source": [
    "labeled_barplot(ds, \"Exited\", perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGnQfFu2kzZe"
   },
   "source": [
    "#### Observations on Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBcn9Xejjx9-"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('______________')               ## Complete the code to create labeled_barplot for Geography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSu6H4hLlBld"
   },
   "source": [
    "#### Observations on Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41FBLuDojx7E"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('______________')               ## Complete the code to create labeled_barplot for Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbDEgCfYlNsi"
   },
   "source": [
    "#### Observations on Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zEQbiJ_jx36"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('______________')               ## Complete the code to create labeled_barplot for Tenure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6PGNDMflTqt"
   },
   "source": [
    "#### Observations on Number of Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWONo8wJjx0K"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('______________')               ## Complete the code to create labeled_barplot for Number of products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WcPAd3sldqS"
   },
   "source": [
    "#### Observations on Has Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxS2uhRYlbyV"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('______________')               ## Complete the code to create labeled_barplot for Has credit card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XFvZ8iZlkeN"
   },
   "source": [
    "#### Observations on Is Active Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuwtR_2Plbvu"
   },
   "outputs": [],
   "source": [
    "labeled_barplot('______________')               ## Complete the code to create labeled_barplot for Is active member"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3hg240IlpqV"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBot6IVnlbtH"
   },
   "outputs": [],
   "source": [
    "# function to plot stacked bar chart\n",
    "\n",
    "\n",
    "def stacked_barplot(data, predictor, target):\n",
    "    \"\"\"\n",
    "    Print the category counts and plot a stacked bar chart\n",
    "\n",
    "    data: dataframe\n",
    "    predictor: independent variable\n",
    "    target: target variable\n",
    "    \"\"\"\n",
    "    count = data[predictor].nunique()\n",
    "    sorter = data[target].value_counts().index[-1]\n",
    "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    print(tab1)\n",
    "    print(\"-\" * 120)\n",
    "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
    "        by=sorter, ascending=False\n",
    "    )\n",
    "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
    "    plt.legend(\n",
    "        loc=\"lower left\",\n",
    "        frameon=False,\n",
    "    )\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw4xLDlemo3C"
   },
   "source": [
    "#### Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qr0qxNDTlbqD"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(_________.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")          # Complete the code to get the heatmap of the data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PslMUrhhnFdl"
   },
   "source": [
    "#### Exited Vs Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMe3Vw9RnDQX"
   },
   "outputs": [],
   "source": [
    "stacked_barplot(ds, \"Geography\", \"Exited\" )           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggqz7d4JnPul"
   },
   "source": [
    "#### Exited Vs Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eckxpn9nnDM9"
   },
   "outputs": [],
   "source": [
    "stacked_barplot('__________')                   ## Complete the code to plot stacked barplot for Exited and Number of follow ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6ch7akdnc9J"
   },
   "source": [
    "#### Exited Vs Has Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb0C_D9rlbnL"
   },
   "outputs": [],
   "source": [
    "stacked_barplot('__________')                   ## Complete the code to plot stacked barplot for Exited and Has credit card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEJVhlwCnj95"
   },
   "source": [
    "#### Exited Vs Is active member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZfHpaIblbkR"
   },
   "outputs": [],
   "source": [
    "stacked_barplot('__________')                   ## Complete the code to plot stacked barplot for Exited and Is active member"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz4mcNb_npGW"
   },
   "source": [
    "#### Exited Vs Credit Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pZBaR7Nlbhd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(y='CreditScore',x='Exited',data=ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX_anKu1oK9L"
   },
   "source": [
    "#### Exited Vs Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YFYCNpeoJDF"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(y='______',x='____',data=______)               ## Complete the code to plot the boxplot for Exited and Age\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdrge12TouUX"
   },
   "source": [
    "#### Exited Vs Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npBbMwsRoJAO"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(y='______',x='____',data=______)               ## Complete the code to plot the boxplot for Exited and Tenure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMvo8Ioco7kd"
   },
   "source": [
    "#### Exited Vs Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtkGjKq6oI9V"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(y='______',x='____',data=______)               ## Complete the code to plot the boxplot for Exited and Balance\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9va9WeGco-Iz"
   },
   "source": [
    "#### Exited Vs Number of Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1XJrS_snCdE"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(y='______',x='____',data=______)               ## Complete the code to plot the boxplot for Exited and Number of products\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cdem_NJpar1"
   },
   "source": [
    "#### Exited Vs Estimated Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCI-CwJppYVv"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.boxplot(y='______',x='____',data=______)               ## Complete the code to plot the boxplot for Exited and Salary\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eWct2L10DUm"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ByhREUF0Hf3"
   },
   "source": [
    "- We want to predict the churn.\n",
    "- Before we proceed to build a model, We'll split the data into train and test to be able to evaluate the model that we build on the train data\n",
    "-  we'll have to encode categorical features.\n",
    "- We will build a model using the train data and then check it's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUXPaUwZHUO8"
   },
   "source": [
    "### Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTb3JwlaHUO-"
   },
   "outputs": [],
   "source": [
    "X = ds.drop(['_____'],axis=1)    ## Complete the code to obtain the independent features into variable X\n",
    "y = ds[['_____']]                ## Complete the code to obtain the dependent features into variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YvyE1kXHUPl"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training and Testing set.\n",
    "\n",
    "X_large, X_test, y_large, y_test = train_test_split(_____,_____, test_size = 0.2, random_state = 42,stratify=y,shuffle = True) ## Complete the code to Split the X and y and obtain test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyMqPCOWc73G"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training and Testing set.\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(_____,_____, test_size = 0.2, random_state = 42,stratify=y_large, shuffle = True) ## complete the code to Split X_large and y_large to obtain train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nEEjgwleiMv"
   },
   "source": [
    "**Create dummy variables for string type variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dsMyKhTc73H"
   },
   "outputs": [],
   "source": [
    "# Encoding Geography variable\n",
    "X_train = pd.get_dummies(X_train, columns=[\"____\",\"____\"],drop_first=True) ## Complete the code to encode Geography and Gender in train set\n",
    "X_test = pd.get_dummies(X_test, columns=[\"____\",\"____\"],drop_first=True)   ## Complete the code to encode Geography and Gender in test set\n",
    "X_val = pd.get_dummies(X_val, columns=[\"____\",\"____\"],drop_first=True)     ## Complete the code to encode Geography and Gender in validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlSyq5fNHUPp"
   },
   "source": [
    "**Normalize the numerical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVx8TYI4c73H"
   },
   "outputs": [],
   "source": [
    "## Complete the below code to scale the data using standardscaler\n",
    "sc=StandardScaler()\n",
    "temp = sc.fit(X_train[[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]])\n",
    "X_train[[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]] = temp.transform(X_train[[\"_____\",\"_____\",\"_____\",\"_____\",\"_____\"]])\n",
    "X_test[[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]] = temp.transform(X_test[[\"_____\",\"_____\",\"_____\",\"_____\",\"_____\"]])\n",
    "X_val[[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"EstimatedSalary\"]] = temp.transform(X_val[[\"_____\",\"_____\",\"_____\",\"_____\",\"_____\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7_uSvumqSMK"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FayG94iciXVS"
   },
   "source": [
    "### Model Evaluation Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ3ZNT7IqXJD"
   },
   "source": [
    "**Model can make wrong predictions as:**\n",
    "* Predicting a customer is exiting  and the customer  is not exiting\n",
    "* Predicting a customer is not exiting and  customer is  exiting\n",
    "\n",
    "**Which case is more important?**\n",
    "* Predicting that customer is not exiting but he/she is exiting. It might cause loss to the banks because due to wrong identification bank will not be able to take any initiative for those sensitive customers. \n",
    "\n",
    "**How to reduce this loss i.e need to reduce False Negative?**\n",
    "* Bank would want `Recall` to be maximized, greater the Recall higher the chances of minimizing false Negative. Hence, the focus should be on increasing Recall or minimizing the false Negative or in other words identifying the True Positive(i.e. Class 1) so that the bank can retain their customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZRg_PVAqoFq"
   },
   "source": [
    "**Create a function for plotting the confusion matrix**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgvtLmPJqklJ"
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwOyQwUh4Mo9"
   },
   "source": [
    "### Model Building: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOVzAE0sc73H"
   },
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aexcSjTuHUPx"
   },
   "outputs": [],
   "source": [
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "# Complete the code to Add the input layer with 64 neurons with relu as activation function with input of 11 variables\n",
    "classifier.add(Dense(activation = '_____', input_dim = 11, units=_____))\n",
    "\n",
    "# Complete the code to add the 1st hidden layer with 32 neurons\n",
    "classifier.add(Dense(_____, activation='relu'))\n",
    "\n",
    "# Add the output layer with one node and sigmoid activation function\n",
    "# we have an output of 1 node, which is the the desired dimensions of our output (stay with the bank or not)\n",
    "# We use the sigmoid because we want probability outcomes\n",
    "classifier.add(Dense(1, activation = '_____')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsCe9B-QHUP9"
   },
   "outputs": [],
   "source": [
    "## Complete the code to Compile the model with SGD optimizer and binary cross entropy as loss with accuracy as metrics\n",
    "classifier.compile(optimizer='_____', loss='_____', metrics=['_____'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1ZM6oGVHUQX"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the summary of the model\n",
    "_____.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5q62XKSHUQc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Complete the code to fit the model on train data for 100 epochs \n",
    "history=classifier.fit(_____, _____,           \n",
    "          validation_data=(X_val,y_val),\n",
    "          epochs=_____,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M55NkPWl6GN1"
   },
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nEKVK07gBlG"
   },
   "outputs": [],
   "source": [
    "# Capturing learning history per epoch\n",
    "hist  = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "# Plotting accuracy at different epochs\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.legend((\"train\" , \"valid\") , loc =0)\n",
    "\n",
    "## Complete the code to evaluate the model on X_test and y_test\n",
    "results = classifier.evaluate(_____, _____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r_0g1NHSGhr"
   },
   "source": [
    "**Confusion matrix** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3aDaCPDRm9T"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the prediction on X_val and obtain the confusion matrix \n",
    "y_pred1=classifier.predict(_____)\n",
    "\n",
    "#Let's predict using default threshold\n",
    "y_pred1 = (y_pred1 > 0.5)\n",
    "cm2=confusion_matrix(y_val, y_pred1)\n",
    "labels = ['True Negative','False Positive','False Negative','True Positive']\n",
    "categories = [ 'Not_Exited','Exited']\n",
    "make_confusion_matrix(cm2, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IciEK79v7GCm"
   },
   "source": [
    "### Model Building: Neural Network model with Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkzWYhkOc73J"
   },
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
    "np.random.seed(2)\n",
    "random.seed(2)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36b7peUubI1q"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "      ## Initializing the neural network\n",
    "      model = Sequential()\n",
    "\n",
    "      ##Complete the code to add the input layer with 64 neurons and relu as activation function\n",
    "      model.add(Dense(_____,activation='_____',input_dim = X_train.shape[1]))\n",
    "\n",
    "      ## Complete the code to add the first hidden layer with 32 neurons with relu as activation functions\n",
    "      model.add(Dense(_____,activation='relu'))\n",
    "\n",
    "      ## Complete the code to add the output layer with Sigmoid Activation\n",
    "      model.add(Dense(1, activation = '_____'))\n",
    "      \n",
    "      ## Complete the code to initialize the model with Adam Optimer\n",
    "      optimizer = tf.keras.optimizers._____(0.001)                                        \n",
    "\n",
    "      ## Complete the code to compile the model with binary cross entropy as loss function and accuracy as metrics\n",
    "      model.compile(loss='_____',optimizer=optimizer,metrics=['accuracy'])\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVs_I0EubWk-"
   },
   "outputs": [],
   "source": [
    "model=create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8j4im_XibZs2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Complete the code to fit the model on train data  \n",
    "history = model.fit(_____,_____,batch_size=32,validation_data=(X_val,y_val),epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMWKudMhrFnb"
   },
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZcVD0PdbjFJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting Train Loss vs Validation Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WEaSCfMrYV-"
   },
   "source": [
    "As you can see from the above image, this model is severely overfitting. Deep learning models are very sensitive to overfitting due to a large number of parameters. We need to find the optimal point where the training should be stopped.\n",
    "\n",
    "The best solution for the above problem is **Early stopping**.\n",
    "\n",
    "**Early stopping:** \n",
    "\n",
    "During training, the model is evaluated on a holdout validation dataset after each epoch. If the performance of the model on the validation dataset starts to degrade or no improvement (e.g. loss begins to increase or accuracy begins to decrease), then the training process is stopped after certain iterations. The model at the time that training is stopped is then used and is known to have good generalization performance.\n",
    "\n",
    "This procedure is called “early stopping” and is perhaps one of the oldest and most widely used forms of neural network regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHUqpfOAnYyt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing classback API \n",
    "# Defining Early stopping\n",
    "es_cb = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)\n",
    "\n",
    "## Complete the code to call the create_model() function\n",
    "model_e=______()\n",
    "\n",
    "## Complete the code to train the model on X_train and y_train for 100 epochs and also use X_val and y_val as validation_data\n",
    "history_e = model_e.fit(_____,_____,batch_size=32,epochs=_____,verbose=1,validation_data=(_____,_____),callbacks=[es_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHY5AVeCbzJg"
   },
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw9wplWKoDlt"
   },
   "outputs": [],
   "source": [
    "# Plotting Train Loss vs Validation Loss\n",
    "plt.plot(history_e.history['loss'])\n",
    "plt.plot(history_e.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lryFo2_jrjx7"
   },
   "source": [
    "**Let's tune the threshold using ROC-AUC**\n",
    "\n",
    "\n",
    "There are many ways we could locate the threshold with the optimal balance between false positive and true positive rates.\n",
    "\n",
    "Firstly, the true positive rate is called the Sensitivity. The inverse of the false-positive rate is called the Specificity.\n",
    "\n",
    "Sensitivity = TruePositive / (TruePositive + FalseNegative)\n",
    "\n",
    "\n",
    "Specificity = TrueNegative / (FalsePositive + TrueNegative)\n",
    "\n",
    "Where:\n",
    "\n",
    "Sensitivity = True Positive Rate\n",
    "\n",
    "Specificity = 1 – False Positive Rate\n",
    "\n",
    "The Geometric Mean or G-Mean is a metric for imbalanced classification that, if optimized, will seek a balance between the sensitivity and the specificity.\n",
    "\n",
    "G-Mean = sqrt(Sensitivity * Specificity)\n",
    "\n",
    "One approach would be to test the model with each threshold returned from the call roc_auc_score() and select the threshold with the largest G-Mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wg5jjr8N68h"
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "yhat = model_e.predict(X_train)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 0]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnLfZ6SXu98p"
   },
   "source": [
    "**Predict the results using  the best  threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX3SgBYSbzWv"
   },
   "outputs": [],
   "source": [
    "#Predicting the results using best as a threshold\n",
    "y_pred_e=model_e.predict(X_val)\n",
    "y_pred_e = (y_pred_e > thresholds[ix])\n",
    "y_pred_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO_RDZZnb9IV"
   },
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ncSz0EIb3Ni"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the classification report Hint: use y_val\n",
    "cr=metrics.classification_report(_____,y_pred_e)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEYayn_Mb_fM"
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Md-l0_EEcEHY"
   },
   "outputs": [],
   "source": [
    "## Complete the code with y_val and y_pred_e to plot the confusion matrix \n",
    "\n",
    "cm1=confusion_matrix(_____, _____)\n",
    "labels = ['True Negative','False Positive','False Negative','True Positive']\n",
    "categories = [ 'Not_Exited','Exited']\n",
    "make_confusion_matrix(cm1, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2TsEYy_pstz"
   },
   "source": [
    "### Model Improvement: Neural Network model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C3Eh9w5c73L"
   },
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
    "np.random.seed(2)\n",
    "random.seed(2)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pm7DGrWQc73L"
   },
   "outputs": [],
   "source": [
    "#Initializing the neural network\n",
    "model_3 = Sequential()\n",
    "\n",
    "#Adding the input layer with 32 neurons and relu as activation function\n",
    "model_3.add(Dense(32,activation='relu',input_dim = X_train.shape[1]))\n",
    "\n",
    "# Complete the code to add dropout with dropout_rate= 0.2\n",
    "model_3.add(Dropout(_____))\n",
    "\n",
    "# Adding the first hidden layer with 16 neurons with relu as activation functions\n",
    "model_3.add(Dense(16,activation='relu'))\n",
    "\n",
    "# Complete the code to add dropout with dropout_rate= 0.1\n",
    "model_3.add(Dropout(_____))\n",
    "# Adding the second hidden layer with 8 neurons with relu as activation functions\n",
    "model_3.add(Dense(8,activation='relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "model_3.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neubbFpzc73L"
   },
   "outputs": [],
   "source": [
    "# Summary of the model\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAxRYgc4c73L"
   },
   "outputs": [],
   "source": [
    "# Initialize the ANN with Adam optimizer \n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "# Complete the code to compile the model with binary cross entropy as loss function and accuracy as metrics\n",
    "model_3.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['______'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkMynB-Uc73L"
   },
   "outputs": [],
   "source": [
    "#Fitting the ANN with batch_size = 32 and 100 epochs \n",
    "history_3 = model_3.fit(X_train,y_train,batch_size=32,epochs=100,verbose=1,validation_data=(X_val,y_val),callbacks=[es_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wyo42PPxrtOx"
   },
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN6vsVZbc73L"
   },
   "outputs": [],
   "source": [
    "# Complete the code to plot the Train Loss and Validation Loss\n",
    "plt.plot(history_3.history['_____'])\n",
    "plt.plot(history_3.history['_____'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kadKNWIc73M"
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "yhat = model_3.predict(X_train)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 0]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iicHziQDuyPK"
   },
   "source": [
    "**Predict the results using  the best  threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJA4CYVtc73M"
   },
   "outputs": [],
   "source": [
    "#Predicting the results using best as a threshold\n",
    "y_pred_e=model_3.predict(X_val)\n",
    "y_pred_3 = (y_pred_e > thresholds[ix])\n",
    "y_pred_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grkQ6kS0rxUx"
   },
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwjMvjfvc73M"
   },
   "outputs": [],
   "source": [
    "# Complete the code to obtain the classification report\n",
    "cr=metrics.classification_report(_____,y_pred_3)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izx7Q2mErz_b"
   },
   "source": [
    "**Confusion report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPw40biRc73M"
   },
   "outputs": [],
   "source": [
    "# Complete the code to obtain the confusion matrix\n",
    "\n",
    "cm1=_____(y_val, y_pred_3)\n",
    "labels = ['True Negative','False Positive','False Negative','True Positive']\n",
    "categories = [ 'Not_Exited','Exited']\n",
    "make_confusion_matrix(cm1, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K90YO0mzOwX8"
   },
   "source": [
    "### Model Improvement: Neural Network model with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2NzU00ac73M"
   },
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
    "np.random.seed(2)\n",
    "random.seed(2)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35OEX3bNyeT3"
   },
   "outputs": [],
   "source": [
    "def create_model_v2(dropout_rate=0.1,lr=0.001,layer_1=64,layer_2=32):  \n",
    "    np.random.seed(1337)\n",
    "\n",
    "    #Initializing the neural network\n",
    "    model = Sequential()\n",
    "\n",
    "    # This adds the input layer (by specifying input dimension)\n",
    "    model.add(Dense(layer_1,activation='relu',input_dim = X_train.shape[1]))\n",
    "\n",
    "    ## Complete the code by adding dropout with dropout rate=0.5\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # # Adding the hidden layer\n",
    "    # Notice that we do not need to specify input dim. \n",
    "    model.add(Dense(layer_2,activation='relu'))\n",
    "\n",
    "    # # Adding the output layer\n",
    "    # Notice that we do not need to specify input dim. \n",
    "    # we have an output of 1 node, which is the the desired dimensions of our output (stay with the bank or not)\n",
    "    # We use the sigmoid because we want probability outcomes\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Adding Adam initializer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    ## Complete the code to compile the model using binary_crossentropy as loss\n",
    "    model.compile(optimizer = optimizer,loss = '_____', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcBG9ypKyeT3"
   },
   "source": [
    "**Using Grid search**\n",
    "\n",
    "We are using grid search to optimize two hyperparameters called **batch size, epochs** due to the limited time. But you can optimize the other hyperparameters as mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S0ezhBqyeT3"
   },
   "outputs": [],
   "source": [
    "## Complete the code by calling the creat_model_v2 function in KerasClassifier\n",
    "keras_estimator = KerasClassifier(build_fn=_____, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1IiggQryeT4"
   },
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {\n",
    "    'batch_size':[40, 64, 128],\n",
    "    \"lr\":[0.01,0.001,0.1]}\n",
    "\n",
    "\n",
    "kfold_splits = 3\n",
    "# Complete the code by using the the defined keras_estimator as estimator.\n",
    "grid = GridSearchCV(estimator=_____,  \n",
    "                    verbose=1,\n",
    "                    cv=kfold_splits,  \n",
    "                    param_grid=param_grid,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wli7_fU3yeT4"
   },
   "outputs": [],
   "source": [
    "## Complete the code to fit the model on train data and use x_val and y_val as validation data\n",
    "grid_result = grid.fit(_____, _____,validation_data = (_____,_____),verbose=1) \n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# Printing mean \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "# Printing standard deviation\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "# Printing best parameters\n",
    "params = grid_result.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWqBmVNdyeT5"
   },
   "source": [
    "**Let's create the final model with the obtained configuration after hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skxGXsQfyeT5"
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "estimator_v2=create_model_v2(lr=grid_result.best_params_['lr'])\n",
    "# Printing model summary\n",
    "estimator_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkc1h61OyeT5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Complete the code to fit the model on train data abd train the model with best batch_size obtained from hyperparameter tuning\n",
    "history_h=estimator_v2.fit(_____, _____, epochs=100, batch_size = grid_result.best_params_['_____'], verbose=1,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c4KjK_nyeT6"
   },
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbATbYWfyeT6"
   },
   "outputs": [],
   "source": [
    "N =100\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(0, N), history_h.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history_h.history[\"val_loss\"], label=\"val_loss\")\n",
    "\n",
    "plt.title(\"Training Loss and Validation loss on the dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"train_Loss/val_loss\")\n",
    "plt.legend(loc=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sorrVB0nsTGS"
   },
   "source": [
    "**Threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihxKDdtg4e5U"
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "yhat = estimator_v2.predict(X_train)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 0]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbn6CItqyeT6"
   },
   "source": [
    "**Predict the results using  the best  threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQJF8Q33yeT6"
   },
   "outputs": [],
   "source": [
    "# Complete the code to obtain the predictions on X_val\n",
    "y_pred_h = estimator_v2.predict(_____)\n",
    "print(y_pred_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM2YrILvyeT7"
   },
   "outputs": [],
   "source": [
    "# To use the confusion Matrix, we need to convert the probabilities that a customer will leave the bank into the form true or false. \n",
    "# So we will use the best cutoff value  to indicate whether they are likely to exit or not.\n",
    "y_pred_h = (y_pred_h > thresholds[ix])\n",
    "print(y_pred_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yk1bcrasf4r"
   },
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05TQdDtvyeT8"
   },
   "outputs": [],
   "source": [
    "# Complete the code to obtain the classification report\n",
    "cr=metrics._____(y_val,y_pred_h)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsuLKdS3yeT7"
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0j7ImQXyeT7"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the confusion matrix\n",
    "cm_h=_____(y_val, y_pred_h)\n",
    "\n",
    "labels = ['True Negative','False Positive','False Negative','True Positive']\n",
    "categories = [ 'Not_Exited','Exited']\n",
    "make_confusion_matrix(cm_h, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TU-l_ltNst3g"
   },
   "source": [
    "### Model Improvement: Neural Network model with balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76LzW0xQODsm"
   },
   "source": [
    "**Now try to apply SMOTE to balance this dataset and then again apply hyperparamter tuning accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaCHzSGvqQNY"
   },
   "outputs": [],
   "source": [
    "## Complete the code to apply SMOTE on train data\n",
    "sm  = _____(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(_____, _____) \n",
    "print('After UpSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
    "print('After UpSampling, the shape of train_y: {} \\n'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtZ8-Ujyq71c"
   },
   "outputs": [],
   "source": [
    "sns.countplot(y_train['Exited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVS6_7yms1Rt"
   },
   "source": [
    "**Build a model with the balanced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0BI30ioc73P"
   },
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
    "np.random.seed(2)\n",
    "random.seed(2)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCBmYTJ6rmoL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Complete the code to initilaize a sequential model\n",
    "model_4 = _____()\n",
    "\n",
    "# Complete the code to add a input layer of 32 neurons with relu as activation function \n",
    "model_4.add(Dense(_____,activation='____',input_dim = X_train.shape[1]))\n",
    "\n",
    "# Complete the code to add dropout with dropout rate=0.2\n",
    "model_4.add(_____(0.2))\n",
    "# Adding hidden layer with 16 neurons with relu as activation function\n",
    "model_4.add(Dense(16,activation='relu'))\n",
    "\n",
    "# Adding the dropout\n",
    "model_4.add(Dropout(0.1))\n",
    "\n",
    "## Complete the code to add dense layers with 8 neurons\n",
    "model_4.add(Dense(_____,activation='relu'))\n",
    "\n",
    "# Complete the code to add suitable activation function in the final layer\n",
    "model_4.add(Dense(1, activation = '_____'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBIIXn76c73P"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the summary of the model\n",
    "model_4._____()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2Zl40pIc73P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Complete the code to define earlystopping as callback\n",
    "es_cb = callbacks._____(monitor='val_loss', min_delta=0.001, patience=5)\n",
    "\n",
    "# Initializing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "# Complining the model with binary cross entropy as loss and accuracy as metrics\n",
    "model_4.compile(optimizer = optimizer,loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Complete the code to fit the model on train with batch_size of 64, epochs of 100 and callbacks as earlystopping\n",
    "history_4 = model_4.fit(_____,_____,batch_size=___,epochs=100,verbose=1,validation_data = (X_val,y_val),callbacks=[_____])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dKyLjt_s6RF"
   },
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXyLsusWjgJi"
   },
   "outputs": [],
   "source": [
    "# Capturing learning history per epoch\n",
    "hist  = pd.DataFrame(history_4.history)\n",
    "hist['epoch'] = history_4.epoch\n",
    "\n",
    "# Plotting accuracy at different epochs\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.legend((\"train\" , \"valid\") , loc =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtvD0AYog9jJ"
   },
   "source": [
    "**Finding the optimal  threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onVWpe2BrCX8"
   },
   "outputs": [],
   "source": [
    "## Complete the code to find the optimal threshold using X_train\n",
    "yhat = model_4.predict(_____)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 0]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09NXOv5usSVj"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the predictions on X_val\n",
    "y_pred_s = model_4.predict(_____)\n",
    "#Predicting the results using tuned threshold\n",
    "y_pred_s = (y_pred_s >thresholds[ix])\n",
    "y_pred_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpyMtq6_tFzR"
   },
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX3pGtousY5l"
   },
   "outputs": [],
   "source": [
    "cr=metrics.classification_report(y_val,y_pred_s)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAzCIUzDtId7"
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJemotBEsdUs"
   },
   "outputs": [],
   "source": [
    "#Calculating the confusion matrix \n",
    "cm_s=confusion_matrix(y_val, y_pred_s)\n",
    "labels = ['True Negative','False Positive','False Negative','True Positive']\n",
    "categories = [ 'Not_Exited','Exited']\n",
    "make_confusion_matrix(cm_s, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpKLOKZdqaQ9"
   },
   "source": [
    "## Final Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oOsU5BYqaQ9"
   },
   "outputs": [],
   "source": [
    "# predict probabilities using the best model\n",
    "yhat = ___________.predict(X_train)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 0]\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(y_train, yhat)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TN_4kWupqaQ-"
   },
   "outputs": [],
   "source": [
    "# Complete the code to Predict the X_test data using the best model\n",
    "y_pred_test = ____________.predict(X_test)\n",
    "y_pred_test = (y_pred_test > thresholds[ix])\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lgrb5OMRt4nv"
   },
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHjfWylLqaQ-"
   },
   "outputs": [],
   "source": [
    "## Complete the code to obtain the classification report on y_test and the predicted values\n",
    "cr=metrics.classification_report(_____,_____)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR8IyY6Nt9HS"
   },
   "source": [
    "**Confusion report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJGMduR6qaQ-"
   },
   "outputs": [],
   "source": [
    "# Complete the code to obtain the confusion matrix \n",
    "cm_h=confusion_matrix(_____, _____)\n",
    "labels = ['True Negative','False Positive','False Negative','True Positive']\n",
    "categories = [ 'Not_Exited','Exited']\n",
    "make_confusion_matrix(cm_h, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE1iHOqqOEmV"
   },
   "source": [
    "## Actionable Insights and Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AHtuEZPvQg9"
   },
   "source": [
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4n5sePPMuEec"
   },
   "source": [
    "_____"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NjPy2L-tqoCo",
    "-cYmf-q8c726",
    "z7ubXtC8HUOA",
    "BwaZDbsYf0-N",
    "2TZ5GstYgG1I",
    "k-Dl42UCgLdV",
    "WP6rrI-kgaxW",
    "tGhe9qVhgoY1",
    "0TTT9Escruwg",
    "0M_y7Y33g2nP",
    "BsBwLHcmHUOg",
    "W036jsgwRdVN",
    "tQlhG1x0iREg",
    "O1WgW83rjza0",
    "MUk3Aj_Bj3cl",
    "9ceasiVRkcjS",
    "6rZ-1bs_ksSe",
    "XWM5q0wQkvws",
    "mGnQfFu2kzZe",
    "xSu6H4hLlBld",
    "xbDEgCfYlNsi",
    "W6PGNDMflTqt",
    "_WcPAd3sldqS",
    "9XFvZ8iZlkeN",
    "d3hg240IlpqV",
    "sw4xLDlemo3C",
    "PslMUrhhnFdl",
    "Ggqz7d4JnPul",
    "U6ch7akdnc9J",
    "qEJVhlwCnj95",
    "tz4mcNb_npGW",
    "MX_anKu1oK9L",
    "Wdrge12TouUX",
    "LMvo8Ioco7kd",
    "9va9WeGco-Iz",
    "7Cdem_NJpar1",
    "7eWct2L10DUm",
    "CUXPaUwZHUO8",
    "W7_uSvumqSMK",
    "FayG94iciXVS",
    "KwOyQwUh4Mo9",
    "IciEK79v7GCm",
    "g2TsEYy_pstz",
    "K90YO0mzOwX8",
    "fcBG9ypKyeT3",
    "TU-l_ltNst3g",
    "G9v-YCu-t6eD",
    "cpKLOKZdqaQ9",
    "XE1iHOqqOEmV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
