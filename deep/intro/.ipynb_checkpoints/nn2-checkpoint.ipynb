{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c1236d-2e33-42a7-b4b3-f6998d9a8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (100, 2) y: (100,)\n",
      "X: (100, 2) y: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] * X[:, 1] > 0).astype(int)\n",
    "print(f\"X: {X.shape} y: {y.shape}\")\n",
    "\n",
    "y = y.reshape(100, 1)\n",
    "\n",
    "print(f\"X: {X.shape} y: {y.shape}\")\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 8\n",
    "hidden_size2 = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)\n",
    "weights_input_hidden1 = np.random.randn(input_size, hidden_size1)\n",
    "bias_hidden1 = np.zeros((1, hidden_size1))\n",
    "weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "bias_hidden2 = np.zeros((1, hidden_size2))\n",
    "weights_hidden2_output = np.random.randn(hidden_size2, output_size)\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative_notused(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "def relu(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "    \n",
    "def relu_derivative(self, dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "    \n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdc3a5-87c9-4e58-a46c-06ce1307948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.18553020691029887\n",
      "Epoch 1000: Loss = 0.2073830546094956\n",
      "Epoch 2000: Loss = 0.3236671189493661\n",
      "Epoch 3000: Loss = 0.2765143769494119\n",
      "Epoch 4000: Loss = 0.629097062964275\n",
      "Epoch 5000: Loss = 0.6413985434672037\n",
      "Epoch 6000: Loss = 0.17847161997800362\n",
      "Epoch 7000: Loss = 0.2647606816107703\n",
      "Epoch 8000: Loss = 0.33548650283476955\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(X, weights_input_hidden1) + bias_hidden1\n",
    "    hidden1_output = sigmoid(hidden1_input)\n",
    "    \n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "    hidden2_output = sigmoid(hidden2_input)\n",
    "    \n",
    "    output = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "    output_prob = sigmoid(output)\n",
    "    #print(f\"output_prob: {output_prob.shape}\")\n",
    "\n",
    "    # Calculate the loss (binary cross-entropy)\n",
    "    loss = -np.mean(y * np.log(output_prob) + (1 - y) * np.log(1 - output_prob))\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = output_prob - y\n",
    "    #print(f\"weights_hidden2_output:{d_output.shape} hidden2_output:{weights_hidden2_output.shape}\")\n",
    "    d_hidden2 = d_output.dot(weights_hidden2_output.T) * sigmoid_derivative(hidden2_output)\n",
    "    d_hidden1 = d_hidden2.dot(weights_hidden1_hidden2.T) * sigmoid_derivative(hidden1_output)\n",
    "\n",
    "    weights_hidden2_output -= hidden2_output.T.dot(d_output) * learning_rate\n",
    "    bias_output -= np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    weights_hidden1_hidden2 -= hidden1_output.T.dot(d_hidden2) * learning_rate\n",
    "    bias_hidden2 -= np.sum(d_hidden2, axis=0, keepdims=True) * learning_rate\n",
    "    weights_input_hidden1 -= X.T.dot(d_hidden1) * learning_rate\n",
    "    bias_hidden1 -= np.sum(d_hidden1, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Generate some test data for predictions\n",
    "X_test = np.array([[1.2, 1.3], [-1.2, -1.3]])\n",
    "\n",
    "# Forward propagation for test data\n",
    "hidden1_input_test = np.dot(X_test, weights_input_hidden1) + bias_hidden1\n",
    "hidden1_output_test = sigmoid(hidden1_input_test)\n",
    "\n",
    "hidden2_input_test = np.dot(hidden1_output_test, weights_hidden1_hidden2) + bias_hidden2\n",
    "hidden2_output_test = sigmoid(hidden2_input_test)\n",
    "\n",
    "output_test = np.dot(hidden2_output_test, weights_hidden2_output) + bias_output\n",
    "output_prob_test = sigmoid(output_test)\n",
    "\n",
    "print(\"Predictions for test data:\")\n",
    "print(output_prob_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f98b3-8c05-4103-8e12-ebfd2d686cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab604bd-e925-475b-b610-f4e3afb0f9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
